{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNMR9gfr7JWYHSNZlAOHAzF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"42a2038ed71849e99681b8763ad50c8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_19907d9bdcf14941ad6c42090225642c","IPY_MODEL_a5cbcc6da7aa4074b55026d6196c4d5f","IPY_MODEL_a8d30ddd01f14236b0bfbbf9e8648730"],"layout":"IPY_MODEL_9b38cde31eb34fc58efb1d2ab611f3c4"}},"19907d9bdcf14941ad6c42090225642c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af179d29a7e2456399c819875230f7f7","placeholder":"​","style":"IPY_MODEL_c1e0e206720746bf9bacbad437bde029","value":"Loading checkpoint shards: 100%"}},"a5cbcc6da7aa4074b55026d6196c4d5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71e12bf2021344059aa026f2c628c565","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_000f6c2a2a484fcfade0929e2549858f","value":2}},"a8d30ddd01f14236b0bfbbf9e8648730":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94c114697dd44b5593ba011ac439ea9e","placeholder":"​","style":"IPY_MODEL_5dbb45b083794cc8bb610481dff4417e","value":" 2/2 [02:11&lt;00:00, 59.91s/it]"}},"9b38cde31eb34fc58efb1d2ab611f3c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af179d29a7e2456399c819875230f7f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1e0e206720746bf9bacbad437bde029":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71e12bf2021344059aa026f2c628c565":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"000f6c2a2a484fcfade0929e2549858f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"94c114697dd44b5593ba011ac439ea9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dbb45b083794cc8bb610481dff4417e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Inferencing\n","\n","A simple inferancing concept with manually inserting the index number from \"test.csv\" annotation file."],"metadata":{"id":"fonJwATiohYP"}},{"cell_type":"code","source":["import os\n","import gc\n","import json\n","import csv\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoTokenizer\n","from huggingface_hub import login\n","from datetime import datetime"],"metadata":{"id":"SSmMB6PoOa_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","gc.collect()\n","\n","# Check for CUDA\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Mount drive to access files\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Define the base directory once\n","BASE_DIR = \"/content/drive/MyDrive/Llama_3B_Instruct_with_Pre-constructed_Prompts\"\n","\n","# Function to generate full paths from base path\n","def path_builder(relative_path):\n","    \"\"\"Returns the full path by combining BASE_DIR with the given relative path.\"\"\"\n","    from pathlib import Path\n","    return str(Path(BASE_DIR) / relative_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5q0--BUhPmrS","executionInfo":{"status":"ok","timestamp":1738882542314,"user_tz":-60,"elapsed":133918,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"9132cc46-bb82-4165-ccea-5f1f06b15f45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Hyperparameter Configuration\n","class Config:\n","    # Model Related\n","    MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n","    ACCESS_TOKEN = \"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\"\n","    CACHE_DIR = path_builder(\"model_cache\")  # Specify cache directory\n","\n","    # Model Matrices\n","    LOG_DIR = path_builder(\"log_dir/\")\n","    CHECKPOINT_DIR = path_builder(\"checkpoints/\")\n","\n","    # Dataset Preprocessing Related\n","    MAX_LENGTH = 2048  # Maximum token sequence length\n","    SAMPLING_FACTOR = 3  # Sampling factor for paraphrasing\n","\n","    # Training Related\n","    BATCH_SIZE = 32\n","    MICRO_BATCH = 1 # Optional\n","    EPOCHS = 5  # Number of initial training epochs\n","\n","    # Optimizer\n","    LEARNING_RATE = 5e-5\n","    WEIGHT_DECAY = 0.01\n","\n","    # Scheduler\n","    S_MODE = 'min'\n","    S_FACTOR = 0.1\n","    S_PATIENCE = 3\n","\n","    # Others\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    ROLE_CLASSES = [\"Antagonist\", \"Protagonist\", \"Innocent\"]  # Main roles\n","    SUBROLE_CLASSES = {\n","        \"Antagonist\": ['Instigator', 'Conspirator', 'Tyrant', 'Foreign Adversary',\n","                       'Traitor', 'Spy', 'Saboteur', 'Corrupt', 'Incompetent',\n","                       'Terrorist', 'Deceiver', 'Bigot'],\n","        \"Protagonist\": ['Guardian', 'Martyr', 'Peacemaker', 'Rebel', 'Underdog', 'Virtuous'],\n","        \"Innocent\": ['Forgotten', 'Exploited', 'Victim', 'Scapegoat']\n","    }\n","\n","# Hugging Face Authentication\n","login(Config.ACCESS_TOKEN)  # Use the access token"],"metadata":{"id":"S8atKJdVP3LM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Function Descriptions"],"metadata":{"id":"EJ0exoKdOWjA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfyxBOrfMpDE"},"outputs":[],"source":["# Model for Classification\n","class EntityClassifier(nn.Module):\n","    def __init__(self, base_model_name: str, token: str, cache_dir: str, freeze_base: bool = True):\n","        \"\"\"\n","        Args:\n","            base_model_name (str): Name of the base model to load from Hugging Face.\n","            token (str): Token for Hugging Face authentication.\n","            cache_dir (str): Directory for caching the model.\n","            freeze_base (bool): Whether to freeze the base model weights during training.\n","        \"\"\"\n","        super(EntityClassifier, self).__init__()\n","        self.main_classes = Config.ROLE_CLASSES  # Use main roles from Config\n","        self.subclasses = Config.SUBROLE_CLASSES  # Use subroles from Config\n","\n","        # Load the base model\n","        self.base_model = AutoModel.from_pretrained(base_model_name, cache_dir=cache_dir,  use_auth_token=token)\n","        self.base_model.gradient_checkpointing_enable()\n","\n","        # Optionally freeze the base model\n","        if freeze_base:\n","            for param in self.base_model.parameters():\n","                param.requires_grad = False\n","\n","        # Define classification heads\n","        hidden_size = self.base_model.config.hidden_size\n","\n","        # Main role classifier\n","        self.main_classifier = nn.Sequential(\n","            nn.Linear(hidden_size, 1024),\n","            nn.LayerNorm(1024),\n","            nn.GELU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(1024, len(self.main_classes))\n","        )\n","\n","        # Subrole classifiers for each main role\n","        self.subrole_classifiers = nn.ModuleDict({\n","            main_class: nn.Sequential(\n","                nn.Linear(hidden_size, 1024),\n","                nn.LayerNorm(1024),\n","                nn.GELU(),\n","                nn.Dropout(0.5),\n","                nn.Linear(1024, len(subroles))\n","            )\n","            for main_class, subroles in self.subclasses.items()\n","        })\n","\n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","        Forward pass for the model.\n","\n","        Args:\n","            input_ids (torch.Tensor): Tokenized input IDs.\n","            attention_mask (torch.Tensor): Attention mask for the inputs.\n","\n","        Returns:\n","            torch.Tensor: Main role logits.\n","            dict: Subrole logits for each main role.\n","        \"\"\"\n","        # Pass input through the base model\n","        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_states = outputs.last_hidden_state\n","\n","        # Use the hidden state of the last token for classification\n","        # Determine the length of each sequence by summing the attention mask\n","        sequence_lengths = attention_mask.sum(dim=1) - 1  # Subtract 1 to get the last token index\n","        cls_representation = hidden_states[torch.arange(hidden_states.size(0)), sequence_lengths]\n","\n","        # Main role predictions\n","        main_role_logits = self.main_classifier(cls_representation)\n","\n","        # Subrole predictions\n","        subrole_logits = {\n","            main_class: classifier(cls_representation)\n","            for main_class, classifier in self.subrole_classifiers.items()\n","        }\n","\n","        return main_role_logits, subrole_logits\n","\n","\n","\n","\n","# Load Model Function\n","def list_available_models_and_tokenizers(save_dir):\n","    \"\"\"\n","    Lists all saved models and tokenizers in the specified directory.\n","\n","    Args:\n","        save_dir: Directory where models and tokenizers are saved.\n","\n","    Returns:\n","        Tuple of lists containing model filenames and tokenizer directory names.\n","    \"\"\"\n","    model_files = [f for f in os.listdir(save_dir) if f.startswith(\"model_checkpoint_\") and f.endswith(\".pth\")]\n","    tokenizer_dirs = [d for d in os.listdir(save_dir) if d.startswith(\"tokenizer_\")]\n","\n","    if not model_files:\n","        raise FileNotFoundError(\"No saved models found in the directory!\")\n","    if not tokenizer_dirs:\n","        raise FileNotFoundError(\"No saved tokenizers found in the directory!\")\n","\n","    print(\"\\nAvailable Models:\")\n","    for i, model in enumerate(model_files, start=1):\n","        print(f\"{i}. {model}\")\n","\n","    print(\"\\nAvailable Tokenizers:\")\n","    for i, tokenizer in enumerate(tokenizer_dirs, start=1):\n","        print(f\"{i}. {tokenizer}\")\n","\n","    return model_files, tokenizer_dirs\n","\n","\n","def load_model_and_tokenizer(save_dir, model_class, device):\n","    \"\"\"\n","    Loads essential layers, tokenizer, and metadata for inference, including thresholds.\n","\n","    Args:\n","        save_dir: Directory where models and tokenizers are saved.\n","        model_class: Class of the model to instantiate.\n","        device: Device to load the model onto (e.g., 'cuda' or 'cpu').\n","\n","    Returns:\n","        Loaded model, tokenizer, metadata (role classes, subrole classes, weights), and thresholds.\n","    \"\"\"\n","    model_files, tokenizer_dirs = list_available_models_and_tokenizers(save_dir)\n","\n","    # Select model and tokenizer\n","    selected_model_index = int(input(\"\\nEnter the number corresponding to the model you want to load: \")) - 1\n","    selected_tokenizer_index = int(input(\"Enter the number corresponding to the tokenizer you want to load: \")) - 1\n","\n","    if selected_model_index < 0 or selected_model_index >= len(model_files):\n","        raise ValueError(\"Invalid model selection!\")\n","    if selected_tokenizer_index < 0 or selected_tokenizer_index >= len(tokenizer_dirs):\n","        raise ValueError(\"Invalid tokenizer selection!\")\n","\n","    selected_model_path = os.path.join(save_dir, model_files[selected_model_index])\n","    selected_tokenizer_path = os.path.join(save_dir, tokenizer_dirs[selected_tokenizer_index])\n","\n","    print(f\"\\nSelected Model: {selected_model_path}\")\n","    print(f\"Selected Tokenizer: {selected_tokenizer_path}\")\n","\n","    # Load essential layers and metadata\n","    checkpoint = torch.load(selected_model_path, map_location=device)\n","    model = model_class().to(device)  # Instantiate your model class\n","    model.main_classifier.load_state_dict(checkpoint['main_classifier'])\n","    for role, state_dict in checkpoint['subrole_classifiers'].items():\n","        model.subrole_classifiers[role].load_state_dict(state_dict)\n","    model.eval()  # Set model to evaluation mode\n","    print(\"\\nEssential model layers loaded successfully!\")\n","\n","    # Load metadata\n","    role_classes = checkpoint.get('role_classes', None)\n","    subrole_classes = checkpoint.get('subrole_classes', None)\n","    role_specific_weights = checkpoint.get('role_specific_weights', None)\n","    optimal_thresholds = checkpoint.get('optimal_thresholds', None)\n","\n","    # Handle missing thresholds\n","    if optimal_thresholds is None:\n","        print(\"No fine-tuned thresholds found. Using default thresholds of 0.5.\")\n","        optimal_thresholds = {role: [0.5] * len(subrole_classes[role]) for role in role_classes}\n","\n","    print(\"Metadata and thresholds loaded successfully!\")\n","\n","    # Load tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(selected_tokenizer_path)\n","    print(\"Tokenizer loaded successfully!\")\n","\n","    return model, tokenizer, role_classes, subrole_classes, role_specific_weights, optimal_thresholds\n","\n","\n","\n","def load_log_file(log_file_path):\n","    \"\"\"\n","    Loads the training log from a specified .json file.\n","\n","    Args:\n","        log_file_path (str): Path to the log file to load.\n","\n","    Returns:\n","        dict: The loaded training log dictionary.\n","    \"\"\"\n","    with open(log_file_path, \"r\") as f:\n","        log = json.load(f)\n","\n","    # Convert loaded log to PyTorch-compatible format if needed\n","    def convert_back_to_tensors(obj):\n","        if isinstance(obj, list):  # Check for tensor-like data (e.g., lists of floats)\n","            try:\n","                return torch.tensor(obj)\n","            except Exception:\n","                return obj  # Return as-is if conversion fails\n","        elif isinstance(obj, dict):  # Recursively process dictionaries\n","            return {k: convert_back_to_tensors(v) for k, v in obj.items()}\n","        else:\n","            return obj  # Leave other types unchanged\n","\n","    processed_log = convert_back_to_tensors(log)\n","\n","    print(f\"Training log loaded from {log_file_path}\")\n","    return processed_log"]},{"cell_type":"markdown","source":["### Load Model Weights, Tokenizer and Logs"],"metadata":{"id":"-4jQwzIPN-XL"}},{"cell_type":"code","source":["!ls \"{path_builder('tuned_model')}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmwZT91nN2Hu","executionInfo":{"status":"ok","timestamp":1738882579940,"user_tz":-60,"elapsed":113,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"bd8e202b-c191-4f58-bfb4-6cb0116cd07b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["model_checkpoint_20250116_193438.pth  tokenizer_20250121_181149\n","model_checkpoint_20250119_102956.pth  tokenizer_20250125_191304\n","model_checkpoint_20250121_181149.pth  training_log_20250125_192245.json\n","model_checkpoint_20250125_191304.pth  training_log_20250125_192351.json\n","tokenizer_20250116_193438\t      training_summary_20250125_191805.txt\n","tokenizer_20250119_102956\n"]}]},{"cell_type":"markdown","source":["**Note:** It's better to load the \"*model_checkpoint_20250125_191304.pth*\" and \"*tokenizer_20250125_191304*\" as it holds the lates tuning. The previous saves may contain inconsistencies as the code base was evolved multiple times during the workflow. Also load the related log file \"*training_log_20250125_192351.json*\"."],"metadata":{"id":"KeL2C9uGvcYl"}},{"cell_type":"code","source":["save_dir = path_builder(\"tuned_model/\")\n","model, tokenizer, role_classes, subrole_classes, role_specific_weights, finetuned_optimal_thresholds = load_model_and_tokenizer(\n","    save_dir=save_dir,\n","    model_class=lambda: EntityClassifier(\n","        base_model_name=Config.MODEL_NAME,\n","        token=Config.ACCESS_TOKEN,\n","        cache_dir=Config.CACHE_DIR,\n","        freeze_base=False  # Adjust based on use case\n","    ),\n","    device=device\n",")\n","\n","# Use thresholds during inference\n","for role, thresholds in finetuned_optimal_thresholds.items():\n","    print(f\"Loaded thresholds for {role}: {thresholds}\")\n","\n","\n","# Load the log\n","loaded_log = load_log_file(log_file_path=path_builder(\"tuned_model/training_log_20250125_192351.json\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":607,"referenced_widgets":["42a2038ed71849e99681b8763ad50c8e","19907d9bdcf14941ad6c42090225642c","a5cbcc6da7aa4074b55026d6196c4d5f","a8d30ddd01f14236b0bfbbf9e8648730","9b38cde31eb34fc58efb1d2ab611f3c4","af179d29a7e2456399c819875230f7f7","c1e0e206720746bf9bacbad437bde029","71e12bf2021344059aa026f2c628c565","000f6c2a2a484fcfade0929e2549858f","94c114697dd44b5593ba011ac439ea9e","5dbb45b083794cc8bb610481dff4417e"]},"id":"YTTGzYTGN56M","executionInfo":{"status":"ok","timestamp":1738882755065,"user_tz":-60,"elapsed":169011,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"5ecce7e3-9223-4bab-cbe0-2957bc111c31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Available Models:\n","1. model_checkpoint_20250116_193438.pth\n","2. model_checkpoint_20250119_102956.pth\n","3. model_checkpoint_20250121_181149.pth\n","4. model_checkpoint_20250125_191304.pth\n","\n","Available Tokenizers:\n","1. tokenizer_20250116_193438\n","2. tokenizer_20250119_102956\n","3. tokenizer_20250121_181149\n","4. tokenizer_20250125_191304\n","\n","Enter the number corresponding to the model you want to load: 4\n","Enter the number corresponding to the tokenizer you want to load: 4\n","\n","Selected Model: /content/drive/MyDrive/NLP_Project/tuned_model/model_checkpoint_20250125_191304.pth\n","Selected Tokenizer: /content/drive/MyDrive/NLP_Project/tuned_model/tokenizer_20250125_191304\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-b2a47ee1d608>:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(selected_model_path, map_location=device)\n","/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a2038ed71849e99681b8763ad50c8e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Essential model layers loaded successfully!\n","Metadata and thresholds loaded successfully!\n","Tokenizer loaded successfully!\n","Loaded thresholds for Antagonist: [0.3899999999999999, 0.4199999999999998, 0.12, 0.43999999999999984, 0.33999999999999986, 0.1, 0.21999999999999995, 0.2799999999999999, 0.19999999999999996, 0.4099999999999998, 0.2899999999999999, 0.12]\n","Loaded thresholds for Protagonist: [0.43999999999999984, 0.5899999999999997, 0.46999999999999986, 0.2899999999999999, 0.3599999999999999, 0.3199999999999999]\n","Loaded thresholds for Innocent: [0.12, 0.5299999999999998, 0.1, 0.5499999999999998]\n","Training log loaded from /content/drive/MyDrive/NLP_Project/tuned_model/training_log_20250125_192351.json\n"]}]},{"cell_type":"markdown","source":["### Example Inferencing\n","\n"," **Note:** Works well for the subroles with higher counts. Please check the \"***NLP_Project_Llama_3B_Instruct.ipynb***\" section \"***2. Data***\" or \"***DP_Prompt_Generation.ipynb***\" for a better understanding of the data distribution."],"metadata":{"id":"Y95g74SxOnuh"}},{"cell_type":"code","source":["import pandas as pd\n","import json\n","import torch\n","\n","# Define the file paths\n","test_file = path_builder(\"Dataset_EN_PT/test_data/test-prompts.json\")\n","csv_file = path_builder(\"Dataset_EN_PT/test_data/test.csv\")\n","\n","# Function to load a single prompt from the JSON test file\n","def load_single_prompt(test_file, index=0):\n","    with open(test_file, 'r', encoding='utf-8') as f:\n","        test_data = [json.loads(line) for line in f]\n","    return test_data[index]['prompt'] if index < len(test_data) else None\n","\n","# Function to map the input index to the correct CSV data row\n","def actual_index_mapping(index, csv_file):\n","    # Read the CSV without changing it\n","    csv_data = pd.read_csv(csv_file)\n","\n","    # In test.csv, the actual data starts from index 2!\n","    # Valid index range starts from 2 and should not exceed the length of the CSV data\n","    if index < 2 or index >= len(csv_data) + 2:\n","        print(\"Invalid index. Please input an index starting from 2.\")\n","        return None\n","    return index - 2  # Adjust index to match CSV data (0-based indexing)\n","\n","# Function to perform inference with thresholds\n","def single_prompt_infer_with_thresholds(prompt, model, tokenizer, device, optimal_thresholds):\n","    inputs = tokenizer(\n","        prompt,\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=Config.MAX_LENGTH,\n","        return_tensors=\"pt\"\n","    )\n","    inputs = {key: val.to(device) for key, val in inputs.items()}\n","\n","    model.eval()\n","    with torch.no_grad():\n","        main_role_logits, subrole_logits = model(inputs['input_ids'], inputs['attention_mask'])\n","\n","    main_role_pred_index = torch.argmax(main_role_logits, dim=1).item()\n","    main_role_pred = Config.ROLE_CLASSES[main_role_pred_index]\n","\n","    role_thresholds = optimal_thresholds.get(main_role_pred, [0.5] * len(Config.SUBROLE_CLASSES[main_role_pred]))\n","    subrole_probs = torch.sigmoid(subrole_logits[main_role_pred]).cpu().numpy()[0]\n","    subrole_preds = [\n","        Config.SUBROLE_CLASSES[main_role_pred][i]\n","        for i, prob in enumerate(subrole_probs)\n","        if prob >= role_thresholds[i]\n","    ]\n","\n","    return {\n","        \"Main Role\": main_role_pred,\n","        \"Subroles\": subrole_preds\n","    }\n","\n","# Load and map the prompt index dynamically\n","index = 273  # Change this to the desired index (2 to 273 in our case)\n","mapped_index = actual_index_mapping(index, csv_file)\n","if mapped_index is not None:\n","    test_prompt = load_single_prompt(test_file, mapped_index)\n","    print(f\"Loaded Test Prompt: {test_prompt}\")\n","\n","    # Perform inference on the selected prompt\n","    result = single_prompt_infer_with_thresholds(test_prompt, model, tokenizer, Config.DEVICE, finetuned_optimal_thresholds)\n","\n","    # Print the result\n","    print(\"\\nInference Result:\")\n","    print(f\"Main Role: {result['Main Role']}\")\n","    print(f\"Subroles: {result['Subroles']}\")\n","else:\n","    print(\"Invalid index. Could not load prompt.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o0JGS4CXOmtg","executionInfo":{"status":"ok","timestamp":1738882818355,"user_tz":-60,"elapsed":5848,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"8127bd22-bc92-474c-9b15-3b86c0aa993b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded Test Prompt: Text:\n","Quais foram as consequências do ataque de Iskander da Rússia contra os Patriot na Ucrânia? O ataque a um comboio de equipamento militar ucraniano na República Popular de Donetsk foi realizado com recurso a mísseis balísticos de curto alcance. As guarnições dos sistemas de defesa aérea Patriot fornecidos à Ucrânia pelos EUA, anteriormente destruídos pelas forças russas, foram \"quase de certeza\" eliminadas, noticiou a revista Forbes. O meio de comunicação social referiu que a \"persistência e a boa sorte\" de um operador russo de drones o \"recompensou no sábado (9), quando localizou um comboio ucraniano incluindo pelo menos dois lançadores quádruplos montados num camião para uma bateria de mísseis terra-ar Patriot\" na República Popular de Donetsk (RPD). \"Os ucranianos perderam nesse dia até 13% dos seus lançadores Patriot\", pelo que \"o ar sobre o Leste da Ucrânia pode ter ficado muito mais seguro para os russos\", segundo a Forbes. O relato surge depois de uma fonte dos serviços de segurança ter dito à Sputnik que dois sistemas Patriot estavam entre o equipamento militar das tropas ucranianas destruído por um ataque do míssil balístico superfície-superfície russo Iskander na RPD. Dados do Ministério da Defesa russo mostraram que o ataque também destruiu um sistema de mísseis S-300 das tropas ucranianas perto da cidade de Pokrovsky, na RPD. A última vez que os militares russos atingiram um Patriot foi no final de fevereiro, quando o lançador, o trator, as munições e o veículo de transporte do sistema foram destruídos por armas de alta precisão. Em maio de 2023, o Ministério da Defesa russo informou que as suas forças tinham destruído um Patriot estacionado em Kiev com um míssil hipersónico Kinzhal. O ministro da Defesa russo, Sergei Shoigu, anunciou um ataque bem-sucedido a outro Patriot duas semanas depois. Os países ocidentais aumentaram os seus fornecimentos militares a Kiev logo após o início da operação militar especial da Rússia em 2022. Moscovo alertou repetidamente contra a continuação das entregas de armas a Kiev, dizendo que isso levaria a uma nova escalada do conflito.\n","\n","Available Roles and Subroles:\n","\n","    Protagonist: Guardian, Martyr, Peacemaker, Rebel, Underdog, Virtuous.\n","    Antagonist: Instigator, Conspirator, Tyrant, Foreign Adversary, Traitor, Spy, Saboteur, Corrupt, Incompetent, Terrorist, Deceiver, Bigot.\n","    Innocent: Forgotten, Exploited, Victim, Scapegoat.\n","\n","Instructions:\n","\n","    - The entity can belong to only one of the three main roles: Protagonist, Antagonist, or Neutral.\n","    - Each main role has its own unique set of subroles. Subroles are specific to the main role and cannot overlap with other main roles.\n","    - The model should output:\n","        - The main role on the first line.\n","        - The subroles (one or more) on the second line.\n","    - No additional text, explanation, or formatting should be provided.\n","\n","Task:\n","Define the role and subroles of 'Moscovo'.\n","\n","\n","Inference Result:\n","Main Role: Protagonist\n","Subroles: ['Guardian']\n"]}]},{"cell_type":"markdown","source":["### Generate Prediction\n","\n","It creates the \"generated_predictions.csv\" which holds all the predicted responses by the model. It is used in \"***Metrics.ipynb***\" during calculation of key metrics such as precision, recall, F1 score, and Exact Match Ratio (EMR) for both main roles and subclasses."],"metadata":{"id":"ecoNKTfkme5B"}},{"cell_type":"code","source":["# Define the file paths\n","test_file = path_builder(\"Dataset_EN_PT/test_data/test-prompts.json\")\n","output_file = path_builder(\"Dataset_EN_PT/generated_predictions.csv\")\n","\n","def load_all_prompts(test_file):\n","    \"\"\"\n","    Load all prompts from the test dataset.\n","\n","    Args:\n","        test_file (str): Path to the test JSON file.\n","\n","    Returns:\n","        list: List of text prompts from the file.\n","    \"\"\"\n","    with open(test_file, 'r', encoding='utf-8') as f:\n","        return [json.loads(line)['prompt'] for line in f]\n","\n","def infer_all_prompts(test_prompts, model, tokenizer, device, optimal_thresholds):\n","    \"\"\"\n","    Perform inference for all prompts using fine-tuned thresholds.\n","\n","    Args:\n","        test_prompts (list): List of text prompts for inference.\n","        model: The trained model for inference.\n","        tokenizer: Tokenizer for preprocessing inputs.\n","        device: The device to run the model on.\n","        optimal_thresholds: Fine-tuned thresholds for subrole predictions.\n","\n","    Returns:\n","        list: List of predictions containing main roles and subroles for each prompt.\n","    \"\"\"\n","    predictions = []\n","    model.eval()\n","    with torch.no_grad():\n","        for prompt in test_prompts:\n","            # Tokenize the input\n","            inputs = tokenizer(\n","                prompt,\n","                padding=\"max_length\",\n","                truncation=True,\n","                max_length=Config.MAX_LENGTH,\n","                return_tensors=\"pt\"\n","            )\n","            inputs = {key: val.to(device) for key, val in inputs.items()}\n","\n","            # Perform inference\n","            main_role_logits, subrole_logits = model(inputs['input_ids'], inputs['attention_mask'])\n","\n","            # Decode predictions\n","            main_role_pred_index = torch.argmax(main_role_logits, dim=1).item()\n","            main_role_pred = Config.ROLE_CLASSES[main_role_pred_index]\n","\n","            # Apply fine-tuned thresholds for subrole predictions\n","            role_thresholds = optimal_thresholds.get(main_role_pred, [0.5] * len(Config.SUBROLE_CLASSES[main_role_pred]))\n","            subrole_probs = torch.sigmoid(subrole_logits[main_role_pred]).cpu().numpy()[0]\n","            subrole_preds = [\n","                Config.SUBROLE_CLASSES[main_role_pred][i]\n","                for i, prob in enumerate(subrole_probs)\n","                if prob >= role_thresholds[i]\n","            ]\n","\n","            # Append the prediction\n","            predictions.append({\n","                \"Main Role\": main_role_pred,\n","                \"Subroles\": subrole_preds\n","            })\n","    return predictions\n","\n","def save_predictions_to_csv(predictions, output_file):\n","    \"\"\"\n","    Save predictions to a CSV file in the required format.\n","\n","    Args:\n","        predictions (list): List of predictions containing main roles and subroles.\n","        output_file (str): Path to save the output CSV file.\n","    \"\"\"\n","    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"Main Role\", \"Subroles\"])  # Header row\n","\n","        for prediction in predictions:\n","            writer.writerow([\n","                prediction[\"Main Role\"],\n","                str(prediction[\"Subroles\"])  # Convert list to string\n","            ])\n","    print(f\"Predictions saved to {output_file}\")\n","\n","# Main workflow\n","if __name__ == \"__main__\":\n","    # Load all prompts\n","    test_prompts = load_all_prompts(test_file)\n","\n","    # Perform inference on all prompts\n","    predictions = infer_all_prompts(\n","        test_prompts,\n","        model,\n","        tokenizer,\n","        Config.DEVICE,\n","        finetuned_optimal_thresholds\n","    )\n","\n","    # Save predictions to CSV\n","    save_predictions_to_csv(predictions, output_file)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZN1aicKOtR6","executionInfo":{"status":"ok","timestamp":1738867817624,"user_tz":-60,"elapsed":205409,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"243b3145-a500-4be6-bddf-f2bc97957200"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions saved to /content/drive/My Drive/NLP_Project/Dataset_EN_PT/generated_predictions.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"IIcO_CZDlhi4"},"execution_count":null,"outputs":[]}]}