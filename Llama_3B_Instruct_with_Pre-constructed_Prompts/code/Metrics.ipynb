{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHs9ylx7zLWVHQH/7c+Gx8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Metrics Calculation Script\n","\n","This code evaluates a multi-label, multi-class classification model by comparing predicted and ground truth data from CSV files. It calculates key metrics such as precision, recall, F1 score, and Exact Match Ratio (EMR) for both main roles and subclasses. The results include detailed performance metrics for each role and subclass, along with average F1 scores and EMR.\n","\n","**Important Notes:**\n","- Our test split already had role and subrole classifications. During test prompt generation, responses containing these roles and subroles were simply discarded (Check the \"***DP_Prompt_Generation.ipynb***\" for more details). Which is why the test.csv can be used as ground truth in this case.\n","- The \"generated_predictions.csv\" file is generated in \"***Inferencing.ipynb***\".  "],"metadata":{"id":"5sRrnKGnvmbK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3UMQRGlUYUm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738883057840,"user_tz":-60,"elapsed":33385,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"6b345577-8439-4bc4-a88d-5613eefffd73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from collections import defaultdict\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from collections import defaultdict\n","import csv\n","import ast\n","\n","# Mount drive to access files\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Define the base directory once\n","BASE_DIR = \"/content/drive/MyDrive/Llama_3B_Instruct_with_Pre-constructed_Prompts\"\n","\n","# Function to generate full paths from base path\n","def path_builder(relative_path):\n","    \"\"\"Returns the full path by combining BASE_DIR with the given relative path.\"\"\"\n","    from pathlib import Path\n","    return str(Path(BASE_DIR) / relative_path)"]},{"cell_type":"code","source":["def evaluate_predictionss(pred_file, ground_truth_file):\n","    # Load the ground truth\n","    ground_truth = {}\n","    with open(ground_truth_file, 'r') as gt_file:\n","        reader = csv.reader(gt_file)\n","        next(reader)  # Skip the header row\n","        counter = 0\n","        for row in reader:\n","            _, _, _, _, main_role, subclasses_str = row  # Adjust indices as per your ground_truth file format\n","            try:\n","                subclasses_list = ast.literal_eval(subclasses_str)  # Safely convert string to list\n","            except Exception as e:\n","                print(f\"Error parsing subclasses_str: {subclasses_str}. Error: {e}\")\n","                subclasses_list = []  # Default to an empty list if parsing fails\n","            ground_truth[counter] = {'main_class': main_role, 'subclasses': subclasses_list}\n","            counter += 1\n","\n","    # Load the predictions\n","    predictions = {}\n","    with open(pred_file, 'r') as pred_file:\n","        reader = csv.reader(pred_file)\n","        next(reader)  # Skip the header row\n","        counter = 0\n","        for row in reader:\n","            main_role, subclasses_str = row\n","            try:\n","                subclasses_list = ast.literal_eval(subclasses_str.strip('\"'))  # Safely convert string to list\n","            except Exception as e:\n","                print(f\"Error parsing subclasses_str: {subclasses_str}. Error: {e}\")\n","                subclasses_list = []  # Default to an empty list\n","            predictions[counter] = {'main_class': main_role, 'subclasses': subclasses_list}\n","            counter += 1\n","\n","    # Initialize metrics\n","    all_main_classes = ['Antagonist', 'Protagonist', 'Innocent']  # Add all main classes here\n","    main_class_to_subclasses = {\n","        'Antagonist': ['Instigator', 'Conspirator', 'Tyrant', 'Foreign Adversary',\n","                       'Traitor', 'Spy', 'Saboteur', 'Corrupt', 'Incompetent',\n","                       'Terrorist', 'Deceiver', 'Bigot'],\n","        'Protagonist': ['Guardian', 'Martyr', 'Peacemaker', 'Rebel', 'Underdog', 'Virtuous'],\n","        'Innocent': ['Forgotten', 'Exploited', 'Victim', 'Scapegoat']\n","    }\n","    all_subclasses = [v for values in main_class_to_subclasses.values() for v in values]\n","\n","    metrics = {\n","        'main_class': {cls: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0} for cls in all_main_classes},\n","        'subclasses': {subcls: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0} for subcls in all_subclasses},\n","        'EMR': 0,\n","        'total_samples': len(ground_truth),\n","    }\n","\n","    # Iterate over ground truth and predictions\n","    for idx, gt_data in ground_truth.items():\n","        gt_main_class = gt_data['main_class']\n","        gt_subclasses = set(gt_data['subclasses'])\n","\n","        pred_data = predictions.get(idx, {'main_class': None, 'subclasses': []})\n","        pred_main_class = pred_data['main_class']\n","        pred_subclasses = set(pred_data['subclasses'])\n","\n","        # Main class metrics\n","        for main_class in all_main_classes:\n","            if gt_main_class == main_class and pred_main_class == main_class:\n","                metrics['main_class'][main_class]['TP'] += 1\n","            elif gt_main_class == main_class and pred_main_class != main_class:\n","                metrics['main_class'][main_class]['FN'] += 1\n","            elif gt_main_class != main_class and pred_main_class == main_class:\n","                metrics['main_class'][main_class]['FP'] += 1\n","            else:\n","                metrics['main_class'][main_class]['TN'] += 1\n","\n","        # Subclass metrics\n","        for subclass in all_subclasses:\n","            if subclass in gt_subclasses and subclass in pred_subclasses:\n","                metrics['subclasses'][subclass]['TP'] += 1\n","            elif subclass in gt_subclasses and subclass not in pred_subclasses:\n","                metrics['subclasses'][subclass]['FN'] += 1\n","            elif subclass not in gt_subclasses and subclass in pred_subclasses:\n","                metrics['subclasses'][subclass]['FP'] += 1\n","            else:\n","                metrics['subclasses'][subclass]['TN'] += 1\n","\n","        # Exact Match Ratio\n","        if gt_main_class == pred_main_class and gt_subclasses == pred_subclasses:\n","            metrics['EMR'] += 1\n","\n","    # Calculate final metrics\n","    results = {'main_class': {}, 'subclasses': {}, 'EMR': metrics['EMR'] / metrics['total_samples']}\n","    for main_class, stats in metrics['main_class'].items():\n","        results['main_class'][main_class] = {\n","            'precision': stats['TP'] / (stats['TP'] + stats['FP']) if stats['TP'] + stats['FP'] > 0 else 0,\n","            'recall': stats['TP'] / (stats['TP'] + stats['FN']) if stats['TP'] + stats['FN'] > 0 else 0,\n","            'f1_score': 2 * stats['TP'] / (2 * stats['TP'] + stats['FP'] + stats['FN']) if 2 * stats['TP'] + stats['FP'] + stats['FN'] > 0 else 0,\n","        }\n","    for subclass, stats in metrics['subclasses'].items():\n","        results['subclasses'][subclass] = {\n","            'precision': stats['TP'] / (stats['TP'] + stats['FP']) if stats['TP'] + stats['FP'] > 0 else 0,\n","            'recall': stats['TP'] / (stats['TP'] + stats['FN']) if stats['TP'] + stats['FN'] > 0 else 0,\n","            'f1_score': 2 * stats['TP'] / (2 * stats['TP'] + stats['FP'] + stats['FN']) if 2 * stats['TP'] + stats['FP'] + stats['FN'] > 0 else 0,\n","        }\n","\n","    return results\n","\n","\n","def main():\n","    pred_file = path_builder(\"Dataset_EN_PT/generated_predictions.csv\")\n","    ground_truth_file = path_builder(\"Dataset_EN_PT/test_data/test.csv\")\n","\n","    results = evaluate_predictionss(pred_file, ground_truth_file)\n","    main_class_f1 = []\n","    subclass_f1 = []\n","\n","    # Print metrics\n","    print(\"Main Class Metrics:\")\n","    for main_class, stats in results['main_class'].items():\n","        print(f\"{main_class}: Precision={stats['precision']:.2f}, Recall={stats['recall']:.2f}, F1 Score={stats['f1_score']:.2f}\")\n","        main_class_f1.append(stats['f1_score'])\n","\n","    print(\"\\nSubclass Metrics:\")\n","    for subclass, stats in results['subclasses'].items():\n","        print(f\"{subclass}: Precision={stats['precision']:.2f}, Recall={stats['recall']:.2f}, F1 Score={stats['f1_score']:.2f}\")\n","        if stats['f1_score'] > 0:\n","            subclass_f1.append(stats['f1_score'])\n","\n","    print(f\"\\nExact Match Ratio (EMR): {results['EMR']:.2f}\")\n","    print(f\"Average Subclass F1 Score: {sum(subclass_f1) / len(subclass_f1):.2f}\")\n","    print(f\"Average Main Class F1 Score: {sum(main_class_f1) / len(main_class_f1):.2f}\")\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRJEXAQGVCjd","executionInfo":{"status":"ok","timestamp":1738883177981,"user_tz":-60,"elapsed":1868,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"0533c83c-62e9-40ed-df53-2fa19dec4a61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Main Class Metrics:\n","Antagonist: Precision=0.71, Recall=0.87, F1 Score=0.78\n","Protagonist: Precision=0.74, Recall=0.59, F1 Score=0.66\n","Innocent: Precision=0.83, Recall=0.67, F1 Score=0.74\n","\n","Subclass Metrics:\n","Instigator: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Conspirator: Precision=0.50, Recall=0.25, F1 Score=0.33\n","Tyrant: Precision=0.23, Recall=0.20, F1 Score=0.21\n","Foreign Adversary: Precision=0.33, Recall=0.19, F1 Score=0.24\n","Traitor: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Spy: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Saboteur: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Corrupt: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Incompetent: Precision=0.14, Recall=0.18, F1 Score=0.16\n","Terrorist: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Deceiver: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Bigot: Precision=0.50, Recall=0.33, F1 Score=0.40\n","Guardian: Precision=0.43, Recall=0.68, F1 Score=0.53\n","Martyr: Precision=1.00, Recall=0.33, F1 Score=0.50\n","Peacemaker: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Rebel: Precision=1.00, Recall=0.17, F1 Score=0.29\n","Underdog: Precision=0.33, Recall=0.33, F1 Score=0.33\n","Virtuous: Precision=0.20, Recall=0.04, F1 Score=0.07\n","Forgotten: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Exploited: Precision=0.00, Recall=0.00, F1 Score=0.00\n","Victim: Precision=0.83, Recall=0.77, F1 Score=0.80\n","Scapegoat: Precision=0.00, Recall=0.00, F1 Score=0.00\n","\n","Exact Match Ratio (EMR): 0.27\n","Average Subclass F1 Score: 0.35\n","Average Main Class F1 Score: 0.73\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_QCPFburnvMi"},"execution_count":null,"outputs":[]}]}