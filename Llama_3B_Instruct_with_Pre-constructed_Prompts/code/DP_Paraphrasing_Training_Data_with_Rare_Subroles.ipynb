{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Paraphrasing to generate more train prompts for rare subroles\n","\n","- This takes in the annotations and the raw files.\n","- Generates 5 different variants of a given text document (in our case raw texts that are used to create prompts)\n","- Stores the variants as JSON to the output directory.\n","\n","Code Reference:\n","- [1] https://github.com/mohan696matlab/Gen-AI-Mini-Projects/blob/main/Paraphraser_app/paraphraser_trial.ipynb"],"metadata":{"id":"TsynbvAbWa6L"}},{"cell_type":"code","source":["import os\n","import re\n","import spacy\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n","from collections import Counter\n","from google.colab import drive\n","from tqdm import tqdm\n","import json\n","import torch\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","# Download NLTK's punkt tokenizer\n","nltk.download('punkt_tab')\n","\n","# Accessing data from drive\n","drive.mount('/content/drive')\n","\n","# Define the base directory once\n","BASE_DIR = \"/content/drive/MyDrive/Llama_3B_Instruct_with_Pre-constructed_Prompts\"\n","\n","# Function to generate full paths from base path\n","def path_builder(relative_path):\n","    \"\"\"Returns the full path by combining BASE_DIR with the given relative path.\"\"\"\n","    from pathlib import Path\n","    return str(Path(BASE_DIR) / relative_path)"],"metadata":{"id":"_F_nAjSFFEyN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738880584420,"user_tz":-60,"elapsed":45142,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"320ccf0b-d8ee-4974-f7e0-1ba454a3b87e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","# Taxonomy Definitions from the PDF\n","ROLES_TAXONOMY = {\n","    \"Protagonist\": [\"Guardian\", \"Martyr\", \"Peacemaker\", \"Rebel\", \"Underdog\", \"Virtuous\"],\n","    \"Antagonist\": [\n","        \"Instigator\", \"Conspirator\", \"Tyrant\", \"Foreign Adversary\", \"Traitor\",\n","        \"Spy\", \"Saboteur\", \"Corrupt\", \"Incompetent\", \"Terrorist\", \"Deceiver\", \"Bigot\"\n","    ],\n","    \"Innocent\": [\"Forgotten\", \"Exploited\", \"Victim\", \"Scapegoat\"]\n","}\n","\n","# Load the paraphrasing model and tokenizer\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n","\n","\n","\n","def paraphrase_sentence(\n","    sentence,\n","    num_beams=10,\n","    num_beam_groups=5,  # Use 1 if enabling sampling\n","    num_return_sequences=5,\n","    max_length=128,\n","    use_sampling=True  # Switch between group beam search and sampling\n","):\n","    \"\"\"\n","    Generate multiple paraphrases for a single sentence using T5.\n","\n","    Args:\n","        sentence (str): The sentence to paraphrase.\n","        num_beams (int): Number of beams for beam search.\n","        num_beam_groups (int): Number of beam groups for diverse beam search.\n","        num_return_sequences (int): Number of paraphrased sequences to return.\n","        max_length (int): Maximum length of the paraphrased text.\n","        use_sampling (bool): Use sampling instead of group beam search.\n","\n","    Returns:\n","        list: A list of paraphrased sentences.\n","    \"\"\"\n","    input_ids = tokenizer(\n","        f'paraphrase: {sentence}',\n","        return_tensors=\"pt\",\n","        padding=\"longest\",\n","        max_length=max_length,\n","        truncation=True\n","    ).input_ids.to(device)\n","\n","    if use_sampling:\n","        # Sampling for diversity\n","        outputs = model.generate(\n","            input_ids,\n","            num_beams=1,                    # Disable beam search\n","            num_return_sequences=num_return_sequences,\n","            max_length=max_length,\n","            do_sample=True,                 # Enable sampling\n","            temperature=1.0,                # Adjust for randomness\n","            no_repeat_ngram_size=2,\n","            repetition_penalty=1.5,\n","        )\n","    else:\n","        # Group Beam Search for diversity\n","        outputs = model.generate(\n","            input_ids,\n","            num_beams=num_beams,\n","            num_beam_groups=num_beam_groups,\n","            num_return_sequences=num_return_sequences,\n","            max_length=max_length,\n","            do_sample=False,                # Disable sampling\n","            diversity_penalty=3.0,          # Encourage diverse outputs\n","            no_repeat_ngram_size=2,\n","            repetition_penalty=1.5,\n","        )\n","\n","    paraphrased = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","    return paraphrased\n","\n","\n","\n","def extract_snippet_with_sentence_check(file_path, start_offset, end_offset, max_sentences=50):\n","    try:\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            text = file.read()\n","\n","        # Tokenize the entire text into sentences\n","        sentences = sent_tokenize(text)\n","\n","        # Find the sentences containing the entity offsets\n","        entity_start_sentence = None\n","        entity_end_sentence = None\n","        for idx, sentence in enumerate(sentences):\n","            sentence_start = text.find(sentence)\n","            sentence_end = sentence_start + len(sentence)\n","            if sentence_start <= start_offset <= sentence_end:\n","                entity_start_sentence = idx\n","            if sentence_start <= end_offset <= sentence_end:\n","                entity_end_sentence = idx\n","\n","        # If sentences are not found, fall back to raw windowed text\n","        if entity_start_sentence is None or entity_end_sentence is None:\n","            snippet_start = max(0, start_offset - 100)\n","            snippet_end = min(len(text), end_offset + 100)\n","            return text[snippet_start:snippet_end].strip()\n","\n","        # Expand context by including neighboring sentences\n","        snippet_start_idx = max(0, entity_start_sentence - max_sentences // 2)\n","        snippet_end_idx = min(len(sentences), entity_end_sentence + max_sentences // 2)\n","\n","        # Combine the selected sentences into a full snippet\n","        snippet = \" \".join(sentences[snippet_start_idx:snippet_end_idx])\n","\n","        return snippet.strip()\n","\n","    except Exception as e:\n","        print(f\"Error processing file {file_path}: {e}\")\n","        return None\n","\n","def add_role_descriptions_and_instructions():\n","    role_descriptions = \"\\n    \".join(\n","        [f\"{role}: {', '.join(subroles)}.\" for role, subroles in ROLES_TAXONOMY.items()]\n","    )\n","    role_descriptions = f\"Available Roles and Subroles:\\n\\n    {role_descriptions}\"\n","\n","    instructions = (\n","        \"Instructions:\\n\\n\"\n","        \"    - The entity can belong to only one of the three main roles: Protagonist, Antagonist, or Neutral.\\n\"\n","        \"    - Each main role has its own unique set of subroles. Subroles are specific to the main role and cannot overlap with other main roles.\\n\"\n","        \"    - The model should output:\\n\"\n","        \"        - The main role on the first line.\\n\"\n","        \"        - The subroles (one or more) on the second line.\\n\"\n","        \"    - No additional text, explanation, or formatting should be provided.\"\n","    )\n","\n","    return role_descriptions, instructions\n","\n","def identify_rare_subroles(annotation_file_path, threshold=20):\n","    annotations = pd.read_csv(annotation_file_path)\n","    role_subrole_counts = {role: {subrole: 0 for subrole in subroles} for role, subroles in ROLES_TAXONOMY.items()}\n","\n","    for _, row in annotations.iterrows():\n","        main_role = row['main_role']\n","        subroles = eval(row['fine_grained_roles'])\n","        if main_role in role_subrole_counts:\n","            for subrole in subroles:\n","                if subrole in role_subrole_counts[main_role]:\n","                    role_subrole_counts[main_role][subrole] += 1\n","\n","    rare_subroles = [\n","        subrole\n","        for subrole_counts in role_subrole_counts.values()\n","        for subrole, count in subrole_counts.items()\n","        if count < threshold\n","    ]\n","    return rare_subroles\n","\n","\n","\n","def create_paraphrase(rare_subroles, annotation_file, raw_documents_folder, output_file=None):\n","    \"\"\"\n","    Generate multiple paraphrased prompts for rare subroles in the dataset.\n","\n","    Args:\n","        rare_subroles (list): List of rare subroles to target.\n","        annotation_file (str): Path to the cleaned annotations CSV file.\n","        raw_documents_folder (str): Path to the folder containing raw text documents.\n","        output_file (str, optional): Path to save the paraphrased prompts as a JSON file.\n","\n","    Returns:\n","        list: A list of dictionaries containing paraphrased prompts and responses.\n","    \"\"\"\n","    paraphrased_prompts = []\n","\n","    annotations = pd.read_csv(annotation_file)\n","\n","    for _, row in tqdm(annotations.iterrows(), total=annotations.shape[0], desc=\"Generating Paraphrases\"):\n","        entity_mention = row['entity_mention']\n","        start_offset = int(row['start_offset'])\n","        end_offset = int(row['end_offset'])\n","        subroles = eval(row['fine_grained_roles'])\n","\n","        if any(rare_subrole in subroles for rare_subrole in rare_subroles):\n","            document_path = os.path.join(raw_documents_folder, row['article_id'])\n","            snippet = extract_snippet_with_sentence_check(document_path, start_offset, end_offset)\n","\n","            if snippet:\n","                sentences = sent_tokenize(snippet)\n","\n","                # Generate multiple paraphrases for each sentence\n","                paraphrased_variants = []\n","                for sentence in sentences:\n","                    paraphrased_sentences = paraphrase_sentence(sentence, num_return_sequences=5)\n","                    paraphrased_variants.append(paraphrased_sentences)\n","\n","                # Flatten the list of lists to get all paraphrased variants\n","                paraphrased_variants_flat = [\" \".join(variant) for variant in zip(*paraphrased_variants)]\n","\n","                role_descriptions, instructions = add_role_descriptions_and_instructions()\n","\n","                for paraphrased_text in paraphrased_variants_flat:\n","                    paraphrased_prompt = (\n","                        f\"Text:\\n{paraphrased_text}\\n\\n\"\n","                        f\"{role_descriptions}\\n\\n\"\n","                        f\"{instructions}\\n\\n\"\n","                        f\"Task:\\nDefine the role and subroles of '{entity_mention}'.\\n\"\n","                    )\n","                    main_role = row['main_role']\n","                    fine_grained_roles = \", \".join(subroles)\n","                    response = f\"Role: {main_role}\\nSubrole(s): {fine_grained_roles}\"\n","                    paraphrased_prompts.append({\"prompt\": paraphrased_prompt, \"response\": response})\n","\n","    if output_file:\n","        pd.DataFrame(paraphrased_prompts).to_json(output_file, orient=\"records\", lines=True)\n","        print(f\"Paraphrased prompts saved to {output_file}\")\n","\n","    return paraphrased_prompts\n","\n"],"metadata":{"id":"BdYLJstw98Fl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737411980964,"user_tz":-60,"elapsed":3158,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"8a66ca58-85e2-463f-bda9-8c72cebe1a61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["annotation_file = path_builder(\"Dataset_EN_PT/train_data/train.csv\")\n","raw_documents_folder = path_builder(\"Dataset_EN_PT/raw-documents_EN_PT\")\n","output_file = path_builder(\"Dataset_EN_PT/train_data/paraphrased_prompts.json\")\n","\n","rare_subroles = identify_rare_subroles(annotation_file, threshold=20)\n","\n","paraphrased_prompts = create_paraphrase(rare_subroles, annotation_file, raw_documents_folder, output_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gQXlGjmEGpT","executionInfo":{"status":"ok","timestamp":1737413340620,"user_tz":-60,"elapsed":1349728,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"9b90b3d4-28d1-4008-87b7-d76a3d987914"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Generating Paraphrases: 100%|██████████| 1385/1385 [22:22<00:00,  1.03it/s]"]},{"output_type":"stream","name":"stdout","text":["Paraphrased prompts saved to /content/drive/My Drive/NLP_Project/Dataset_EN_PT/train_data/paraphrased_prompts.json\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Just some checks."],"metadata":{"id":"z2nPKn8zHQi6"}},{"cell_type":"code","source":["def paraphrased_prompts_stats(json_file_path):\n","    \"\"\"\n","    Check the number of prompts, responses, and subrole-specific counts in a JSON file.\n","\n","    Args:\n","        json_file_path (str): Path to the JSON file containing prompts and responses.\n","\n","    Returns:\n","        dict: A dictionary containing stats on total entries, unique prompts, unique responses, and subrole counts.\n","    \"\"\"\n","    try:\n","        # Load the JSON file into a DataFrame\n","        data = pd.read_json(json_file_path, orient=\"records\", lines=True)\n","\n","        # Count total entries, unique prompts, and unique responses\n","        total_entries = len(data)\n","        unique_prompts = data['prompt'].nunique()\n","        unique_responses = data['response'].nunique()\n","\n","        # Extract subroles from the 'response' field and count them\n","        subrole_counter = Counter()\n","        for response in data['response']:\n","            # Extract the subroles portion of the response (after \"Subrole(s):\")\n","            if \"Subrole(s):\" in response:\n","                subroles_text = response.split(\"Subrole(s):\")[-1].strip()\n","                subroles = [subrole.strip() for subrole in subroles_text.split(\",\")]\n","                subrole_counter.update(subroles)\n","\n","        # Return the stats\n","        return {\n","            \"Total Entries\": total_entries,\n","            \"Unique Prompts\": unique_prompts,\n","            \"Unique Responses\": unique_responses,\n","            \"Subrole Counts\": dict(subrole_counter)\n","        }\n","    except Exception as e:\n","        print(f\"Error reading or processing the JSON file: {e}\")\n","        return None\n"],"metadata":{"id":"vhXhPHjTEm5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["json_file_path = path_builder(\"Dataset_EN_PT/train_data/paraphrased_prompts.json\")\n","\n","counts = paraphrased_prompts_stats(json_file_path)\n","if counts:\n","    print(\"Summary of JSON File:\")\n","    print(f\"Total Entries: {counts['Total Entries']}\")\n","    print(f\"Unique Prompts: {counts['Unique Prompts']}\")\n","    print(f\"Unique Responses: {counts['Unique Responses']}\")\n","    print(\"\\nSubrole Counts:\")\n","    for subrole, count in counts['Subrole Counts'].items():\n","        print(f\"{subrole}: {count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_MuViLjpHLEY","executionInfo":{"status":"ok","timestamp":1737467192697,"user_tz":-60,"elapsed":1706,"user":{"displayName":"Sudipta Barman","userId":"03096804389780096641"}},"outputId":"88c672ef-42fa-49a0-b311-3bc122325d74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Summary of JSON File:\n","Total Entries: 385\n","Unique Prompts: 385\n","Unique Responses: 14\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"APLDdOR4HV0q"},"execution_count":null,"outputs":[]}]}